\documentclass[11pt]{article}

\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{cite}

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who are hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}

\title{Nearest Neighbor Search in General Metric Spaces} 


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{Neel Rakholia,$^{1\ast}$ William March,$^{2}$ George Biros$^{2}$\\
\\
\normalsize{$^{1}$Department of Applied Mathematics and Applied Physics,}\\
\normalsize{Columbia University,}
\normalsize{New York, NY 10027, USA}\\
\normalsize{$^{2}$Institute for Computational Engineering and Science,}\\
\normalsize{University of Texas at Austin,}
\normalsize{Austin, TX 78705, USA}\\
\\
\normalsize{$^\ast$To whom correspondence should be addressed; E-mail:  nvr2105@columbia.edu}
}

% Include the date command, but leave its argument blank.
\date{}

\begin{document}

% Double-space the manuscript.
%\baselineskip24pt
% Make the title.
\maketitle 

\paragraph{Abstract.} Significant work has been done on addressing the problem of nearest neighbor (NN) search in Euclidean Space. Notable is the wealth of literature on search techniques involving the use of trees. Kd-trees and Ball trees are among the more commonly used data structures to speed up NN search. Surprisingly little work however has been done on using trees to find nearest neighbors in general metric spaces.
\\
\indent In this paper we undertake a study of vantage point trees (VP-trees), and analyze their effectiveness in finding NN for kernel based distance metrics. We also propose two methods for searching these trees: an exact backtrack search algorithm, and an approximate random tree search algorithm. Previous work on VP-trees has focused on a priority queue based search, which is not very effective for even moderately high dimensional data. Our approach is unique in this regard.
\\
\indent For an RBF-kernel based distance metric, 500,000 training points and 54 features, NN search on 20,000 query points using the random trees search method yielded 90 percent accuracy with only about 1 percent of total distance evaluations: an improvement of about 2 orders of magnitude. 

\paragraph{Keywords.} Nearest Neighbor Algorithms, Tree Codes, Metric Spaces, Data Analysis, VP-Trees, Machine Learning

\section{Introduction} 
\paragraph{} The nearest neighbor problem refers to finding the set of points $P_c$ in a database of points $D$ that are closest to a query point $q$. The notion of close and correspondingly distance can be fairly arbitrary as long as it follows the following properties. For a space to be metric, these properties must hold true. ~\cite{Nayar08}
\begin{enumerate}
\item Reflectivity: $d(a,a) = 0$
\item Symmetry: $d(a,b) = d(b,a)$
\item Non-Negativity: $d(a,b) > 0$, $a \neq b$
\item Triangle Inequality: $d(a, b) \leq d(a, c) + d(b, c)$
\end{enumerate}

\paragraph{} One of the more widely used distance metrics are derived from similarity measures such a kernels. These are of immense practical importance in fields such Computer Vision and Natural Language Processing where the concept of similarity between abstract objects is of importance. A kernel $K:$ $\mathbb{R}^d \times \mathbb{R}^d$ is a similarity function with the property that for any $x$ and $y$, the distance between $x$ and $y$ increases, $K(x, y)$ decreases. The construction of the kernel distance subsequently involves a transformation from similarities to distances. It can be represented in the following general form. Given two “objects” $A$ and $B$, and a measure of similarity between them given by $K(A,B)$, then the induced distance between $A$ and $B$ can be defined as the difference between the self-similarities $K(A,A) + K(B,B)$ and the cross-similarity $K(A,B)$. Additionally this distance could be normalized by taking the square root. ~\cite{Phillips10}

\begin{equation}
d(A,B) = \sqrt{K(A,A) + K(B,B) - 2K(A,B)}
\end{equation}

\paragraph{} VP-trees require the use of a bounded distance metric: a metric that yields distance between $[0,1]$. Any unbounded kernel distance metric can be scaled to be a bounded metric by the following simple transformation: ~\cite{Yianilos93}

\begin{equation}
d'(A,B) = \frac{d(A,B)}{1 + d(A,B)}
\end{equation}

\paragraph*{Significance} 

Vantage point trees (VP-trees) use concentric hyperspheres to partition points into a metric tree. ~\cite{Yianilos93} 

\bibliography{reportbib}{}
\bibliographystyle{plain}

\end{document}